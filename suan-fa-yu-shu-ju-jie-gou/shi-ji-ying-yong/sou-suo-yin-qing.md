# 搜索引擎

![1584098469133](../../.gitbook/assets/1584098469133.png)

搜索引擎大致可以分为四个部分：搜集、索引、查询、链接分析。其中，

* 搜集：利用爬虫爬取网页。
* 索引：一篇一篇地读取并记住哪些单词出现在哪些文档中。
* 查询：主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。
* 链接分析：利用文档间的链接对文档的重要性进行排序，这就是PageRank。

## 搜集

搜索引擎把整个互联网看作数据结构中的**有向图**，**把每个页面看作一个顶点**。如果某个页面中包含另外一个页面的链接，那我们就在两个顶点之间连一条有向边。这样我们就可以利用图的遍历搜索算法，来遍历整个互联网中的网页。

搜索引擎采用广度优先搜索策略遍历网页，而不是深度优先（BFS得到的网页权重都比较高，DFS得到的网页权重降低得比较快）。具体实现时，先找一些比较知名的网页（权重比较高）的链接（比如新浪主页网址、腾讯主页网址等），作为种子网页链接，放入到队列中。爬虫按照广度优先的策略，不停地从队列中取出链接，然后去爬取对应的网页，解析出网页里包含的其他网页链接，再将这些链接添加到队列中。

在实现层面，就有存储待爬取网页链接、url判重、存储爬取到的网页等主要细节。

### links.bin：存储待爬取网页链接

在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从links.bin文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到links.bin文件中。

这样用文件来存储网页链接的方式，还有其他好处。比如，**支持断点续爬**。当机器断电之后，网页链接不会丢失；当机器重启之后，还可以从之前爬取到的位置继续爬取。

关于如何解析页面获取链接，相当于把整个页面看作一个大的字符串，利用字符串匹配算法，在这个大字符串中，搜索网页标签（例如`<a href="XXX">`），然后顺序读取之间的字符串。

### bloom\_filter.bin：网页判重

使用布隆过滤器，可以快速并且非常节省内存地实现网页的判重。但是，如果我们把布隆过滤器存储在内存中，那机器宕机重启之后，布隆过滤器就被清空了。这样就可能导致大量已经爬取的网页会被重复爬取。

定期地（比如每隔半小时）将布隆过滤器持久化到磁盘中，存储在`bloom_filter.bin`文件中。这样，即便出现机器宕机，也只会丢失布隆过滤器中的部分数据。当机器重启之后，我们就可以重新读取磁盘中的`bloom_filter.bin`文件，将其恢复到内存中。

### doc\_raw.bin：存储原始网页

爬取到网页之后，需要将其存储下来，以备后面离线分析、索引之用。

如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。所以，我们可以**把多个网页存储在一个文件中**，这就是所谓的Big Files。每个网页之间，通过一定的标识进行分隔，方便后续读取。

在大文件中，使用3个维度描述每个页面：

* 同步码：一个页面数据的开始
* 长度
* 压缩包：页面本身及其相关信息

![1584100064914](../../.gitbook/assets/1584100064914.png)

当然，这样的一个文件也不能太大，因为文件系统对文件的大小也有一定的限制。所以，我们可以设置每个文件的大小不能超过一定的值（比如1GB）。随着越来越多的网页被添加到文件中，文件的大小就会越来越大，当超过1GB的时候，我们就创建一个新的文件，用来存储新爬取的网页。

### doc\_id.bin：映射网页链接到ID

给每个网页分配一个唯一的ID，相比用url表征一个网页，ID方便我们后续对网页进行分析、索引。

可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将url跟ID之间的对应关系，存储在另一个`doc_id.bin`文件中。

## 索引

为了创建倒排索引，需要先对爬取下来的网页进行离线分析，包括**抽取网页文本信息**和**分词并创建临时索引**。

### 抽取网页文本信息

网页是半结构化数据（按照HTML语法规范书写），里面夹杂着各种标签、JavaScript代码、CSS样式。对于搜索引擎来说，它只关心网页中的文本信息，也就是，网页显示在浏览器中时，能被用户肉眼看到的那部分信息。

所以可以依靠HTML标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步：

1. 去掉JavaScript代码、CSS格式以及下拉框中的内容（因为下拉框在用户不操作的情况下，也是看不到的）。也就是`<style></style>, <script></script>, <option></option>`这三组标签之间的内容。可以利用**AC自动机**这种多模式串匹配算法，在网页这个大字符串中，一次性查找`<style>, <script>, <option>`这三个关键词，当找到某个关键词出现的位置之后，依次往后遍历，直到对应结束标签为止，这期间遍历到的字符串连带着标签就可以从网页中删除。
2. 去掉所有HTML标签，也可以通过字符串匹配算法来实现，做法类似上一步。

### 分词并创建临时索引

经过上面的处理之后，我们就可以对从网页中抽取出的文本信息进行分词，并且创建正向索引。

对于英文网页来说，分词非常简单。我们只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。

对于中文网页，可以用比较简单的**基于字典和规则的分词**方法。

> 字典也叫词库，里面包含大量常用的词语。
>
> 借助词库并采用**最长匹配规则**，来对文本进行分词。
>
> 比如要分词的文本是“中国人民解放了”，我们词库中有“中国”“中国人”“中国人民”“中国人民解放军”这几个词，那我们就取最长匹配，也就是“中国人民”划为一个词，而不是把“中国”、“中国人“划为一个词。
>
> 具体到实现层面，我们可以将词库中的单词，构建成Trie树结构，然后拿网页文本在Trie树中匹配。

每个网页的文本信息在分词完成之后，可以得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（`tmp_Index.bin`），这个临时索引文件用来构建倒排索引文件。在临时索引文件中，我们不存储单词本身，而是存储单词的ID，以节省存储空间，ID的生成与文档ID的生成类似。临时索引文件的格式如下：

![1584102018925](../../.gitbook/assets/1584102018925.png)

除了这个索引文件，还使用散列表，记录已经编过号的单词。在对网页文本信息分词的过程中，对于分割出来的单词，先到散列表中查找，如果找到，那就直接使用已有的编号；如果没有找到，我们再去计数器中拿号码，并且将这个新单词以及编号添加到散列表中。

当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，命名为`term_id.bin`。

### 构建倒排索引

倒排索引（ Inverted index）记录了每个单词以及包含它的网页列表。

![1584102514793](../../.gitbook/assets/1584102514793.png)

在临时索引文件中，记录的是单词跟每个包含它的文档之间的对应关系。如何通过临时索引文件，构建出倒排索引文件呢？

考虑到临时索引文件很大，无法一次性加载到内存中，所以一般会选择使用**多路归并排序**的方法来实现。

先对临时索引文件，按照单词编号的大小进行排序。因为临时索引很大，所以一般基于内存的排序算法就没法处理这个问题了。可以用归并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，可以直接利用MapReduce来处理。

临时索引文件排序完成之后，相同的单词就被排列到了一起。我们只需要顺序地遍历排好序的临时索引文件，就能将每个单词对应的网页编号列表找出来，然后把它们存储在倒排索引文件中。

![1584102638388](../../.gitbook/assets/1584102638388.png)

除了倒排文件，我们还需要一个文件，来记录**每个单词编号在倒排索引文件中的偏移位置**，把这个文件命名为`term_offset.bin`。这个文件的作用是，帮助我们快速地查找某个单词编号在倒排索引中存储的位置，进而快速地从倒排索引中读取单词编号对应的网页编号列表。

![1584103862369](../../.gitbook/assets/1584103862369.png)

## 查询

利用之前产生的几个文件，来实现最终的用户搜索功能。

* `doc_id.bin`：记录url和ID之间的对应关系
* `term_id.bin`：记录单词和ID之间的对应关系
* `index.bin`：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表
* `term_offsert.bin`：记录每个单词编号在倒排索引文件中的偏移位置

这四个文件中，除了倒排索引文件`index.bin`比较大之外，其他的都比较小。为了方便快速查找数据，我们将其他三个文件都加载到内存中，并且组织成散列表这种数据结构。

当用户在搜索框中，输入某个查询文本的时候，我们先对用户输入的文本进行分词处理。假设分分词之后，我们得到k个单词。拿这k个单词，去`term_id.bin`对应的散列表中，查找对应的单词编号。经过这个查询之后，我们得到了这k个单词对应的单词编号。

我们拿这k个单词编号，去`term_offset.bin`对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。经过这个查询之后，我们得到了k个偏移位置。拿这k个偏移位置，去倒排索引`index.bin`中，查找k个单词对应的包含它的网页编号列表。经过这一步查询之后，我们得到了k个网页编号列表。

针对这k个网页编号列表，统计每个网页编号出现的次数，可以借助散列表来进行统计。统计得到的结果，我们按照出现次数的多少，从小到大排序。出现次数越多，说明包含越多的用户查询单词（用户输入的搜索文本，经过分词之后的单词）。

经过这一系列查询，我们就得到了一组排好序的网页编号。我们拿着网页编号，去`doc_id.bin`文件中查找对应的网页链接，分页显示给用户就可以了。

