# 归一化和标准化

## 为什么需要特征变换？

监督式学习会根据某个样本的一系列特征，最后判定它应该属于哪个**分类**，并给出一个离散的分类标签。除此之外，还有一类监督式学习算法，会根据一系列的特征输入，给出**连续的预测值**。

举个例子，房地产市场可以根据销售的历史数据，预估待售楼盘在未来的销售情况。如果只是预估卖得“好”还是“不好”，那么这个粒度明显就太粗了。如果我们能做到预估这些房屋的售价，那么这个事情就变得有价值了。想要达成这个预测目的的过程，就需要最基本的**因变量连续回归分析**。

将$y_{1}$，$y_{2}$，…，$y_{i}$称为因变量，$x_{1}$，$x_{2}$，…，$x_{k}$称为自变量。通常情况下，因变量的值可以分解为两部分。

* 一部分是受自变量影响的，即表示为自变量相关的函数，其中函数形式已知，可能是线性也可能是非线性函数，但包含一些未知参数
* 另一部分是由于其他未被考虑的因素和随机性的影响，即随机误差。

如果因变量和自变量为线性关系时，就称为**线性回归模型**；如果因变量和自变量为非线性关系，则称为**非线性回归分析模型**。

其中，多元线性回归的基本形式是：

![img](../.gitbook/assets/1350fcaad0a241fae13896bf85fa4d70.png)

其中，$x_{1}$，$x_{2}$，…，$x_{n}$是自变量，$y$是因变量，$ε$是随机误差，通常假定随机误差的均值为0。而w0是截距，$w_{1}$，$w_{2$，…，$w_{n}$是每个自变量的系数，表示每个自变量对最终结果的影响是正面还是负面，以及影响的程度。如果某个系数大于0，表示对应的自变量对结果是正面影响，这个自变量越大，结果就越大。否则就是负面影响，这个自变量越大，结果就越小。而系数的绝对值表示了影响程度的大小，如果绝对值趋于0，表示基本没有影响。

但是在正式开始线性回归分析之前，还存在一个问题：不同字段的数据没有可比性。这就需要用到特征值变换了。

## 归一化

获取原始数据的最大值和最小值，然后把原始值线性变换到\[0,1\]之间，具体的变换函数为：

$$x^{'}=\frac {x-min}{max - min}$$

其中$x$是原始值，$max$为样本数据的最大值，$min$为样本数据的最小值，$x’$是变换后的值。这种方法有个不足：**最大值与最小值非常容易受噪音数据的影响**。

## 标准化

基于正态分布的z分数（z-score）标准化（Standardization）。该方法**假设数据呈现标准正态分布**。

z分数标准化是利用标准正态分布的特点，计算一个给定分数距离平均数有多少个标准差。它的具体转换公式如下：

$$x^{'}=\frac {x-\mu}{\sigma}$$

其中$x$为原始值，$u$为均值，$σ$为标准差，$x’$是变换后的值。

经过z分数的转换，高于平均数的分数会得到一个正的标准分，而低于平均数的分数会得到一个负的标准分数。更重要的是，转换后的数据是符合标准正态分布（均值为0，标准差为1）的。

和归一化相比，z分数这样的标准化不容易受到噪音数据的影响，并且保留了各维特征对目标函数的影响权重。

