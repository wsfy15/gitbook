# 马尔可夫模型

## 马尔可夫模型

只要序列的每个状态之间存在转移的概率，那么我们就可以使用马尔科夫模型。有时候情况会更复杂，不仅每个状态之间的转移是按照一定概率进行的，就连每个状态本身也是按照一定概率分布出现的，那么还需要用到隐马尔科夫模型。

**每个词按照一定的概率转移到下一个词**。

如果把词抽象为一个状态，那么我们就可以认为，状态到状态之间是有关联的。前一个状态有一定的概率可以转移到到下一个状态。如果多个状态之间的随机转移满足马尔科夫假设，那么这类随机过程就是一个马尔科夫随机过程。而刻画这类随机过程的统计模型，就是**马尔科夫模型**（Markov Model）。

对于二元文法来说，某个词出现的概率只和前一个词有关。对应的，在马尔科夫模型中，如果一个状态出现的概率只和前一个状态有关，那么称它为**一阶马尔科夫模型**或者**马尔科夫链**。对应于三元、四元甚至更多元的文法，也有二阶、三阶等马尔科夫模型。

最简单的**马尔科夫模型——马尔科夫链**：

![img](../.gitbook/assets/90537ae6b49b65b154d2084e6e8385b9.jpg)

我们可以根据某个应用的需要，把上述状态转移表具体化。例如，对于语言模型中的二元文法模型：

![img](../.gitbook/assets/90796185dff6955b48cb300ea11839f4.png)

除了二元文法模型，**马尔科夫链**还有很多应用的场景。PageRank算法它的核心思想就是基于马尔科夫链。

这个算法假设了一个“随机冲浪者”模型，冲浪者从某张网页出发，根据Web图中的链接关系随机访问。在每个步骤中，冲浪者都会从当前网页的链出网页中随机选取一张作为下一步访问的目标。在整个Web图中，绝大部分网页节点都会有链入和链出。那么冲浪者就可以永不停歇地冲浪，持续在图中走下去。

在随机访问的过程中，**越是被频繁访问的链接，越是重要**。可以看出，每个节点的PageRank值取决于Web图的链接结构。假如一个页面节点有很多的链入链接，或者是链入的网页有较高的被访问率，那么它也将会有更高的被访问概率。

那么，PageRank的公式和马尔科夫链有什么关系呢？先看一张Web的拓扑图。

![img](../.gitbook/assets/842598a61f0edc622552544603dbe9bf.jpg)

其中A、B、C等结点分别代表了页面，而结点之间的有向边代表了页面之间的超链接。我们可以假设每张网页就是一个状态，而网页之间的链接表明了状态转移的方向。这样，我们很自然地就可以使用马尔科夫链来刻画“随机冲浪者”。

另外，在最基本的PageRank算法中，我们可以假设每张网页的出度是$n$，那么从这张网页转移到任何下一张相连网页的概率都是$\frac{1}{n}​$，因此这个转移的概率只和当前页面有关，满足一阶马尔科夫模型的假设。在之前的拓扑结构中添加转移的概率。

![img](../.gitbook/assets/ea12a997c4dff97a31991021860d0c5d.jpg)

PageRank在标准的马尔科夫链上，引入了随机的跳转操作，也就是假设冲浪者不按照Web图的拓扑结构走下去，只是随机挑选了一张网页进行跳转。这样的处理是类比人们打开一张新网页的行为，也是符合实际情况的，避免了信息孤岛的形成。最终，根据马尔科夫链的状态转移和随机跳转，可以得到如下的PageRank公式。

![img](../.gitbook/assets/9afb3366f6b4d4c9aeeebc7e59e96bb2.png)

其中，$p_{i}$表示第$i$张网页，$M_{i}$是$p_{i}$的入链接集合，$p_{j}$是$M_{i}$集合中的第$j$张网页。$PR_{\(p_{j}\)}$表示网页$p_{j}$的PageRank得分，$L_{\(p_{j}\)}$表示网页$p_{j}$的出链接数量，$\frac{1}{L_{\(p_{j}\)}}$就表示从网页$p_{j}$跳转到$p\_{i}$的概率。$α$是用户不进行随机跳转的概率，$N$表示所有网页的数量。

从最简单的马尔科夫链，到多阶的马尔科夫模型，它们都可以刻画基于马尔科夫假设的随机过程，例如概率语言模型中的多元文法和PageRank这类链接分析算法。但是，这些模型都是假设每个状态对我们都是已知的，比如在概率语言模型中，每个状态都对应了一个单词“上学”。可是，有没有可能某些状态我们是未知的呢？

## 隐马尔科夫模型

以语音识别为例，机器只能得到一串发音，但不知道具体怎么写，相当于计算机**只能观测到每个状态的部分信息，而另外一些信息被“隐藏”了起来**。

这个时候，就需要用隐马尔科夫模型来解决这种问题。隐马尔科夫模型有两层，一层是可以观测到的数据，称为“输出层”，另一层则是无法直接观测到的状态，称为“隐藏状态层”。考虑了**状态之间转移的概率**和**状态产生输出的概率**。

![img](../.gitbook/assets/77593998432b6290808a80c63b830f75.jpg)

其中，$x_{1}，x_{2}，x_{3}$等等属于隐藏状态层，$a_{12}$表示了从状态$x_{1}$到$x_{2}$的转移概率，$a_{23}$表示了从状态$x_{2}$到$x_{3}$的转移概率。这一层和普通的马尔科夫模型是一致的，可惜在隐马尔科夫模型中我们无法通过数据直接观测到这一层。我们所能看到的是，$y_{1}，y_{2}，y_{3}$等等代表的“输出层”。另外，$b_{11}$表示了从状态$x_{1}$到$y_{1}$的输出概率，$b_{22}$表示了从状态$x_{2}$到$y_{2}$的输出概率，$b_{33}$表示了从状态$x_{3}$到$y\_{3}$的输出概率等等。

在这个两层模型示例中，“隐藏状态层”产生“输出层”的概率是

![img](../.gitbook/assets/80bd25ad15e9d852cbea8b4ef1f4a67a.png)

### 应用

以普通话语音识别为例，计算机接收到下面词组的发音：

```text
xiang(四声)mu(四声) kai(一声)fa(一声) shi(四声)jian(四声)
```

假设根据我们手头上的语料数据，这个词组有多种可能，我列出两种。

* ![img](../.gitbook/assets/f7df10338a0fda0fbecc9046fc6ea388.jpg)

  三个确定的状态是“项目”“开发”和“时间”这三个词。 从“项目”转移到“开发”的概率是0.25，从“开发”转移到“时间”的概率是0.3。 从“项目”输出“xiang（三声）mu（四声）”的概率是0.1，输出”xiang（四声）mu（四声）”的概率是0.8，输出“xiang（四声）mu（一声）”的概率是0.1，“开发”和“时间”也有类似的输出概率。

  > “项目”的普通话发音就是“xiang（四声）mu（四声）”，为什么还会输出其他的发音呢？
  >
  > 这是因为，前面说的这些概率都是通过历史语料的数据统计而来。在进行语音识别的时候，我们会通过不同地区、不同性别、不同年龄等等的人群，采集发音的样本。如此一来，影响这个发音的因素就很多了，比如方言，口音、误读等等。

![img](../.gitbook/assets/7fd2cf482bbd849ff0dfb64d90e3a9f4.png)

* ![img](../.gitbook/assets/d9e15463a4a6d6e227f614da19d2c601.jpg)

  ![img](../.gitbook/assets/c91f4ed6837cd11e6231c237a2ab9707.png)

最后比较第一种和第二种情况产生的概率，分别是P\(项目\)x0.0027和P\(橡木\)x0.000459。假设P\(项目\)和P\(橡木\)相等，那么“项目开发时间”这个词组的概率更高。

**流程：**

1. 根据拼音去找到单个对应的词语，**不考虑声调的概率**。
2. 再根据词语之间转移的概率，词语对应目标音高的概率，进而求出整个句子输出的概率。概率越大，可能性越高。

