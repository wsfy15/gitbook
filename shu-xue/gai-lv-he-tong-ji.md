# 概率和统计

## 概率

* **随机变量**：描述事件所有可能出现的状态
  * **离散型随机变量**
  * **连续型随机变量**
* **概率分布**：描述每个状态出现的可能性
* **联合概率** P\(x, y\)
* **边缘概率** P\(x\)：
  * 对于离散型随机变量，通过联合概率P\(x, y\)在y上求和
  * 对于连续型随机变量，通过联合概率P\(x, y\)在y上的积分
* **条件概率**
* **信息论**
  * **信息熵**（Entropy）/ **香农熵**（Shannon Entropy）
  * **信息增益**（Information Gain）
  * **基尼指数**（Gini）

**概率和统计其实是互逆的。**概率论是对数据产生的过程进行建模，然后研究某种模型所产生的数据有什么特性。而统计学则需要通过已知的数据，来推导产生这些数据的模型是怎样的。

### 概率分布

随机变量的每种取值的出现都遵从一定的可能性，把这个可能性用具体的数值表示出来就是**概率**。如果将随机变量所有可能出现的值，及其对应的概率都罗列出来，就能获得这个变量的概率分布。

#### 离散

抛硬币事件，出现正面和反面的概率都是50%。$$ $$

**分布模型**：

* 伯努利分布：

  ​ 单个随机变量的分布，而且这个变量的取值只有两个，0或1。伯努利分布通过参数λ来控制这个变量为1的概率。

  $$ P\(x=0\)=1-λ $$

  $$ P\(x=1\)=λ ​$$

  $P\(x\)=λ^{x}\(1-λ\)^{1-x}​$

  抛硬币就属于伯努利分布。

* 分类分布（Categorical Distribution）:

  ​ 也叫Multinoulli分布，描述了一个具有k（有限值）个不同状态的单个随机变量。

  $P\(x=k\)=λ\_{k}$

* 二项分布 
* 泊松分布
* ……

#### 连续

车速随时间的变化。

**分布模型**

* 正态分布（Normal Distribution）

  $P\(x\) = \frac{1}{\sqrt{2πσ^{2}}}exp\left\( -\frac{\(x-μ\)^{2}}{2σ^{2}} \right\)$ \(μ表示均值，σ表示方差\)

  ![img](../.gitbook/assets/b514674826625cfec19bbe3bfec89e82.png)

  越靠近中心点μ，出现的概率越高，而随着渐渐远离μ，出现的概率先是加速下降，然后减速下降，直到趋近于0。蓝色区域上的数字，表示了这个区域的面积，也就是数据取值在这个范围内的概率。例如，数据取值在\[-1σ, μ\]之间的概率为34.1%。

  一元标准正态分布：μ为0，σ为1

* 均匀分布
* 指数分布
* 拉普拉斯分布

### 期望值

也叫数学期望，是**每次随机结果的出现概率乘以其结果的总和**。

例如计算多个数值的平均值，其实就是求期望值，只不过我们假设每个数值出现的概率是相同的。

一个问题只要满足两个要素，就可以考虑使用期望值：

* 在这个问题中可能出现不同的情况，而且各种情况的出现**满足了一定的概率分布**
* 每种情况都对应一个数值，这个数值代表了具体的应用含义

离散值的期望很容易求，就是概率乘值。

连续值的期望则通过积分求，相反的，通过微分可以求某个值出现的概率。

### 联合概率

以班级的成绩分布表为例：

![img](../.gitbook/assets/cf93c9001c0638f0e85c033a3d9aa23d.png)

这张表中有两个随机变量，一个是学生的性别，一个是分数区间。

男生的概率是$P\(男生\)=10/20=50%$，90分及以上的学生的概率是$P\(90-100\)=4/20=20%$。全班考了90分以上的男生的概率是$P\(男生, 90-100\)=2/20=10%​$，这个概率由性别和分数这两个随机变量同时决定。这种由多个随机变量决定的概率就叫**联合概率，它的概率分布就是联合概率分布**。随机变量x和y的联合概率使用P\(x, y\)表示。

**联合概率和单个随机变量的概率之间的关联**：

* 对于离散型随机变量，可以通过通过联合概率P\(x, y\)在y上求和，就可以得到P\(x\)。
* 对于连续型随机变量，可以通过联合概率P\(x, y\)在y上的积分，推导出概率P\(x\)。
* 这个时候，我们称P\(x\)为**边缘概率**。

### 条件概率

条件概率也是由多个随机变量决定，但是和联合概率不同的是，它计算了**给定某个（或多个）随机变量的情况下，另一个（或多个）随机变量出现的概率**，其概率分布叫做**条件概率分布**。给定随机变量x，随机变量y的条件概率使用P\(y \| x\)表示。

在男生中（给定性别这个随机变量的范围），考90分及以上的概率是$P\(90-100\|男生\)= 2/10=20%$。

### 贝叶斯定理

#### 概率、条件概率和联合概率 间的关系

联合概率是条件概率和概率的乘积：

$P\(x,y\)=P\(x\mid y\)\*P\(y\)​$

$P\(x,y\)=P\(y\mid x\)\*P\(x\)​$

知道 全班男女比例、考90～100分的概率，以及90～100分中男生的概率，求 男生考了90～100分的概率有多少。

$P\(x,y\)=P\(x\mid y\)_P\(y\)=P\(y\mid x\)_P\(x\)​$

$P\(x \mid y\) = \frac{P\(y\mid x\)\*P\(x\)}{P\(y\)}​$

这里随机变量x就是 考了90 ~ 100分，随机变量y是 性别是男。

* P\(x\)：**先验概率**，因为它是从数据资料统计得到的，不需要经过贝叶斯定理的推算。
* P\(y \| x\)：在统计学中，称之为似然函数L\(x \| y\)。似然函数和概率是有区别的。
  * 概率是指已经知道模型的参数来预测结果
  * 似然函数是根据观测到的结果数据，来预估模型的参数。
  * 不过，当y值给定的时候，两者在数值上是相等的
* P\(y\)：没有必要事先知道。可以通过联合概率P\(x, y\)计算边缘概率得来，而联合概率P\(x, y\)可以由P\(y\|x\) \* P\(x\)推出。
* P\(x\|y\)：**后验概率**，是根据贝叶斯定理，通过先验概率P\(x\)、似然函数P\(y \| x\)和边缘概率P\(y\)推算而来。

### 随机变量之间的独立性

随机变量相互独立，则$P\(x \mid y\) = P\(x\)$、$P\(y \mid x\) = P\(y\)$。

$P\(x,y\)=P\(x\)\*P\(y\)$

## 朴素贝叶斯

**朴素**：假设各随机变量之间相互独立。

以水果分类为例，假设有下列三种水果及一些相关属性。

| 水果名称 | 形状 | 外观颜色 | 斑马纹理 | 重量 | 握感 | 囗感 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 苹果 | 不规则圆 | 红色 | 无 | 200.45克 | 较硬 | 酸甜 |
| 甜橙 | 圆形 | 橙色 | 无 | 150.92克 | 较软 | 甜 |
| 西爪 | 椭圆形 | 绿色 | 条纹 | 6000.88克 | 较硬 | 甜 |

首先需要将这些属性转换为计算机可读的

![img](../.gitbook/assets/8ce159eb36bbed80bda6ab7f6d10bc5d.png)

（重量 由连续值转化为离散值，因为**朴素贝叶斯处理的都是离散值**）。

扩展训练样本

![img](../.gitbook/assets/e0f92302ec5f9a7353177644db88a408.png)

如果现在有一个新的水果，它也有一定的形状、颜色、口感等等，怎么判断它是哪种水果呢？

**利用贝叶斯定理，用先验概率和条件概率估计后验概率**。

$P\(c \mid f\) = \frac{P\(f\mid c\)\*P\(c\)}{P\(f\)}​$ （c表示分类，f表示属性）

等号左边的$P\(c\|f\)$就是待分类样本中，出现属性值f时，样本属于类别c的概率。而等号右边的$P\(f\|c\)$是根据训练数据统计，得到分类c中出现属性f的概率。P\(c\)是分类c在训练数据中出现的概率，P\(f\)是属性f在训练样本中出现的概率。

这里的贝叶斯公式只描述了单个属性值属于某个分类的概率，如果要分析的水果有很多属性，怎么做？

基于 变量间相互独立的假设，可以得到

![img](../.gitbook/assets/51a8c96cd9a8ae06acdff25a87438332.png)

（o是一个拥有$f\_i$ 和 $f\_j$ 属性的水果）

根据训练样本，可以得到统计结果

![img](../.gitbook/assets/504f97b994046fc3fee82690acdd5622.png)

**平滑**：对于0概率，需要用一个比数据集中最小非零概率还小的极小值代替，例如0.01。

假设有一个新的水果，它的形状是圆形，口感是甜的，那么根据朴素贝叶斯，它属于苹果、甜橙和西瓜的概率分别是

![img](../.gitbook/assets/4b9b7fc57f85344a90fde596b58ef110.png)

![img](../.gitbook/assets/490bc17c05070918564c20b943daed1e.png)

比较这三个值，可以发现该水果是甜橙的概率最大。

这里的概率乘积都非常小，如果属性很多时，概率可能就趋近0了，因此在实际中，会采用一些数学手法进行转换，例如取log将小数转换为绝对值大于1的负数。

### 步骤总结

* 准备数据：针对水果分类这个案例，我们收集了若干水果的实例，并从水果的常见属性入手，将其转化为计算机所能理解的数据。这种数据也被称为**训练样本**。
* 建立模型：通过手头上水果的实例，我们让计算机统计每种水果、属性出现的**先验概率**，以及在某个水果分类下某种属性出现的**条件概率**。这个过程也被称为**基于样本的训练**。
* 分类新数据：对于一颗新水果的属性数据，计算机根据已经建立的模型进行推导计算，得到该水果属于每个分类的概率，实现了分类的目的。这个过程也被称为**预测**。

### 与其他分类算法比较

* **KNN**：朴素贝叶斯需要更多的时间进行模型的训练，但是它在对新的数据进行分类预测的时候，通常效果更好、用时更短。
* **决策树**：朴素贝叶斯并不能提供一套易于人类理解的规则，但是它可以提供决策树通常无法支持的模糊分类（一个对象可以属于多个分类）
* **SVM支持向量机**：朴素贝叶斯无法直接支持连续值的输入。

如果一个分类的应用场景中，待分类对象的属性值**大部分都是离散的**（或者很容易转化为离散的）、需要支持**模糊分类**，并且需要**快速可靠**的实时分类，那么这种场景通常就非常适合使用朴素贝叶斯方法。

## 基于朴素贝叶斯的文本分类

App推送用户感兴趣的新闻，一个非常重要的步骤就是给新闻分类。

### **基本框架**

1. **采集训练样本。**先给新闻打标签（政治、军事、财经、体育等），预留一部分用于测试。
2. **预处理自然语言。**包括 词包（bag of words）、分词、词干（Stemming）和归一化（Normalization）、停用词（Stopword）、同义词（Synonyms）和扩展词处理等，使机器能够理解文本。
3. **训练模型。**获取每个分类的先验概率、每个属性的先验概率以及给定某个分类时，出现某个属性的条件概率。
4. **实时分类预测。**根据训练阶段所获得的先验概率和条件概率，来预估给定一系列属性的情况下属于某个分类的后验概率。

### 预处理

文本的的重要属性是什？

假如说，有人给你一篇几千字的文章，让你在10秒钟之内说出文章大意，你会怎么办？我想大部分人的解决方案是“**找关键词**”！我们也可以交给计算机用同样的办法。而计算机处理文本的基本单位就是字和单词，这就是人们最常用的方法：词袋（Bag of words）模型。

这种模型会忽略文本中的词语出现的顺序以及相应的语法，将整篇文章仅仅看做是一个大量单词的组合。所有单词相互之间是独立的，这个假设和朴素贝叶斯模型的独立假设是一致的。

#### 分词

* **基于字符串匹配**：正向最大匹配、逆向最大匹配、长词优先。只需使用基于字典的匹配，因此计算复杂度低；缺点是处理歧义词效果不佳。
* **基于统计和机器学习**：**隐马尔科夫模型**（Hidden Markov Model），**条件随机场**（Conditional Random Field）。基于人工标注的词性和统计特征，对中文进行建模。训练阶段，根据标注好的语料对模型参数进行估计。 在分词阶段再通过模型计算各种分词出现的概率，将概率最大的分词作为最终结果。

#### 取词干和归一化

取词干的目标就是为了减少词的变化形式，将派生词转化为基本形式。

```text
将am，is，are，was，were全部转换为be
将car，cars，car’s，cars’全部转换为car
```

归一化则是大小写转化和多种拼写（例如color和colour）的统一化。

#### 停用词

停用词：不影响（或基本不影响）相关性的词，可以将这些词过滤掉，提高计算机处理的效率。

要注意停用词的使用场景，例如用户观点分析，good和bad这样的形容词反而成为了关键。不仅不能过滤，反而要加大它们的权重。

#### 同义词和扩展词

维护同义词词典：

```text
番茄，西红柿
菠萝，凤梨
洋山芋，土豆
泡面，方便面，速食面，快餐面
山芋，红薯
鼠标，滑鼠
……
```

当出现“番茄”关键词时，将其等同于“西红柿”。

如果简单地将Dove分别和多芬、德芙简单地等价，那么多芬和德芙这两个完全不同的品牌也变成了同义词，这样做明显是有问题的。

可以采用扩展关系，当系统看到文本中的“多芬”时将其等同于“Dove”，看到“德芙”时将其等同于“Dove”。但是看到“Dove”的时候并不将其等同于“多芬”或“德芙”。这就是扩展词。

### 模型训练

通过词包模型的假设，以及上述这些自然语言处理的方法，我们可以将整篇的文字，切分为一个个的单词，这些是表示文章的关键属性。每个单词可以作为文章的属性，而通过这些单词的词频（出现的频率），我们很容易进行概率的统计。下面对分类的先验概率、单词的先验概率、某个分类下某个单词的条件概率分别给出示例。

![img](../.gitbook/assets/204896aa4fd777102d579764cbf08d90.png)

![img](../.gitbook/assets/83baf008f9888a6afb6ec5720270aaf3.png)

给定分类，单词出现的条件概率：

![img](../.gitbook/assets/7ed70239e88b4fffaaa7d26e93305877.png)

某些单词从未在某个分类中出现，对于这种情况，可以使用**平滑**，将其词频或条件概率设置为一个极小的值（例如航母在体育和娱乐分类下）。

### 预测

![img](../.gitbook/assets/51a8c96cd9a8ae06acdff25a87438332-1579144532443.png)

在新闻分类中，**o**就表示一篇文章，而**c**表示新闻的种类（包括政治、军事、财经等等）。而属性字段**f**就是我们从文档集而建立的各种单词。公式等号左边的**P\(c\|f\)**就是待分类新闻中，出现单词f时，该新闻属于类别c的概率。而等号右边的P\(f\|c\)是根据训练数据统计，得到分类c中出现单词f的概率。P\( c \)是分类c在新闻训练数据中出现的概率，P\(f\)是单词f在训练样本中出现的概率。

用刚才表格中的数据来计算“中国航母”这个短语属于每个分类的概率。

![img](../.gitbook/assets/1b0c4ffb16a1cca9f3a3f211a7a69dd1.png)

![img](../.gitbook/assets/dc38e39930981fd69a4021a4b9fda4ef.png)

![img](../.gitbook/assets/24420c243fe5c2f439654596be6f2c8d.png)

![img](../.gitbook/assets/f4bc1a5f6737296e49385ae9c809465a.png)

可以看出，“中国航母”这个短语本身属于“政治”和“军事”分类的可能性最高，而属于“财经”的可能性最低。（在真正的实现中，需要将中文词和中文分类名称转换为数字型的ID，以提高系统的效率）

考虑更多的单词：

![img](../.gitbook/assets/929ef79bdb208063ad744c285e51231e.png)

这个概率乘积会趋近于0，因此需要转换（取log等）。

## 语言模型

### 链式法则

使用一系列条件概念率和边缘概率，来推导联合概率。

![img](../.gitbook/assets/de5d37d9392a18225b4f9d522b4f180c.png)

### 马尔可夫假设

任何一个词$w\_{i}$出现的概率只和它前面的1个或若干个词有关。

N-gram模型，“N”表示任何一个词出现的概率，只和它前面的N-1个词有关。

以二元文法模型为例，某个单词出现的概率只和它前面的1个单词有关。

![img](../.gitbook/assets/b0bc4bb97ba4b233e0ae9db0291059e9.png)

如果是三元文法，就说明某个单词出现的概率只和它前面的2个单词有关。

![img](../.gitbook/assets/a619504d5c698f0c519904cd6eddf094.png)

一元文法：

![img](../.gitbook/assets/523ff06450418d4d91d8e6d2756e1025.png)

假设我们有一个统计样本文本$d$，$s$表示某个有意义的句子，由一连串按照特定顺序排列的词$w_{1}，w_{2},…,w\_{n}$组成，这里$n$是句子里单词的数量。

根据文档d的统计数据，$s$在文本中出现的可能性，即$P\(s\|d\)$，那么可以把它表示为$P\(s\|d\)=P\(w_{1}, w_{2}, …, w_{n} \| d$\)。假设这里考虑的都是在集合d的情况下发生的概率，所以可以忽略$d$，写为$P\(s\)=P\(w_{1}, w_{2}, …, w_{n}\)$。

计算$P\(w_{1}, w_{2}, …, w\_{n}\)$，要在集合中找到一模一样的句子，基本是不可能的。这个时候，就需要使用链式法则把这个式子改写为：

![img](../.gitbook/assets/2fe51ac1be8efbad2082627860aada23.png)

但是上式$P\(w_{3}\|w_{1},w_{2}\)$出现概率很低，$P\(w_{4}\|w_{1},w_{2},w_{3}\)$出现的概率就更低了。一直到$P\(w_{n}\|w_{1}, w_{2}, …, w\_{n-1}\)$，基本上又为0了。

$P\(w_{1}, w_{2}, …, w_{n}\)和P\(w_{n}\|w_{1}, w_{2}, …, w\_{n-1}\)$不只会导致0概率，它还会使得模型存储空间的急速增加。

如果我们**使用三元文法模型**，上述公式可以改写为：

![img](../.gitbook/assets/6dfe1f3c267787677090a611fee964b1.png)

这样，系统的复杂度大致在\(C\(m, 1\) + C\(m, 2\) + C\(m, 3\)\)这个数量级，而且$P\(w_{n}\|w_{n-2}, w_{n-1}\)$为0的概率也会大大低于$P\(w_{n}\|w_{1}, w_{2}, …, w\_{n-1}\)$ （其中$n&gt;&gt;3$）为0的概率。当然，多元文法模型中的N还是不能太大。随着N的增大，系统复杂度仍然会快递升高，就无法体现出多元文法的优势了。

### 信息检索

信息检索很关心的一个问题就是相关性，也就是说，给定一个查询，哪篇文档是更相关的呢？

布尔模型和向量空间检索模型都是从查询的角度出发，观察查询和文档之间的相似程度，而语言模型则采用概率。

一种常见的做法是计算$P\(d\|q\)$，其中$q$表示一个查询，$d$表示一篇文档。$P\(d\|q\)$表示用户输入查询$q$的情况下，文档$d$出现的概率是多少？如果这个概率越高，我们就认为$q$和$d$之间的相关性越高。

通过贝叶斯定理，可以将$P\(d\|q\)$重写如下：

$P\(d \mid q\) = \frac{P\(q\mid d\)\*P\(d\)}{P\(q\)}$

对于同一个查询，其出现概率$P\(q\)$都是相同的，同一个文档$d$的出现概率$P\(d\)$也是固定的。因此它们可以忽略，我们只要关注如何计算$P\(q\|d\)$。

![img](../.gitbook/assets/d78ba0db38a575269eff4f6cb89afa66.png)

为了提升效率，使用马尔科夫假设和多元文法。假设是三元文法：

![img](../.gitbook/assets/e977fb4b682e67a8693f1b76f1febe13.png)

最终，当用户输入一个查询$q$之后，对于每一篇文档$d$，我们都能获得$P\(d\|q\)$的值。根据每篇文档所获得的$P\(d\|q\)$这个值，由高到低对所有的文档进行排序。这就是语言模型在信息检索中的常见用法。

### 中文分词

假设整个文档集合是D，要分词的句子是$s$，分词结果为$w_{1}$, … $w_{n}$，那么$P\(s\)$的概率为：

![img](../.gitbook/assets/c74b222e54b6c2a9a6385a988896f334.png)

根据链式法则和三元文法模型，重写上面的式子：

![img](../.gitbook/assets/d4cfe338d5aed842cd1b366008a551be.png)

> 在信息检索中，我们关心的是**每篇文章**产生一个句子（也就是查询）的概率，而这里可以是**整个文档集合**D产生一个句子的概率。

语言模型可以帮我们**估计某种分词结果，在文档集合中出现的概率**。但是由于不同的分词方法，会导致$w_{1}$到$w_{n}$的不同，因此就会产生不同的$P\(s\)$。接下来，我们只要取最大的$P\(s\)$，并假设这种分词方式是最合理的，就可以在一定程度上解决歧义。我们可以使用$arg maxP\(W_{i} \mid D\)$这个公式来求解（$W_{i}$表示第$i$种分词方法\)。

